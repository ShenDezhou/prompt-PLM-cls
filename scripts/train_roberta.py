# -*- coding: utf-8 -*-
"""rct-roberta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L_BBtLDu7kDzSZj6SPbvwPgXzhcDMDLi
"""

import os
os.chdir('/content/drive/MyDrive')
!git clone https://github.com/ShenDezhou/rct-roberta-plm

import os
os.chdir('/content/drive/MyDrive/rct-roberta-plm')
!git pull

os.chdir('/content/drive/MyDrive/rct-roberta-plm/scripts/')
!sh setup.sh

import os
print(os.environ["COLAB_TPU_ADDR"])
import torch
# imports the torch_xla package
import torch_xla
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.data_parallel as dp
import torch_xla.distributed.xla_multiprocessing as xmp
num_cores = 8
devices = (
   xm.get_xla_supported_devices(max_devices=num_cores) if num_cores==8 else [])
print("Devices: {}".format(devices))

import os
os.chdir('/content/drive/MyDrive/rct-roberta-plm')
!sh scripts/train.sh